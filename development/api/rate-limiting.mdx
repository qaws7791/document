---
title: "API 속도 제한"
description: "API 속도 제한(Rate Limiting)은 API 요청 수를 제어하여 서버 자원 보호, 서비스 안정성 및 공정한 사용을 보장하는 필수 메커니즘입니다."
---

## HTTP API 속도 제한

HTTP API는 현대 애플리케이션 아키텍처의 핵심 요소로, 다양한 서비스와 클라이언트 간의 상호작용을 가능하게 합니다. 하지만 아무런 제한 없이 API 요청을 허용한다면, 특정 사용자의 과도한 요청이나 악의적인 공격(예: DDoS)으로 인해 서버 자원이 고갈되고 전체 서비스가 마비될 수 있습니다. **API 속도 제한(Rate Limiting)**은 이러한 문제를 방지하기 위해 단위 시간당 API 요청 수를 제어하는 필수적인 메커니즘입니다.


### 왜 Rate Limiting을 구현해야 할까요? (필요성 및 장점)

API에 Rate Limiting을 적용하는 것은 다음과 같은 중요한 이유 때문입니다.

1. **서버 자원 보호:** 특정 클라이언트나 사용자가 과도한 요청으로 서버의 CPU, 메모리, 데이터베이스 연결 등 중요 자원을 독점하거나 고갈시키는 것을 방지하여 서비스의 안정성을 확보합니다.
2. **서비스 거부(DoS/DDoS) 공격 완화:** 비정상적으로 많은 요청을 보내는 악의적인 공격 트래픽을 제한하여 서비스 가용성을 유지하는 데 중요한 방어 수단이 됩니다. 완벽한 방어는 아니지만 기본적인 보호 계층을 제공합니다.
3. **공정한 사용 보장:** 모든 사용자 또는 클라이언트가 API 리소스에 공평하게 접근할 수 있도록 보장합니다. 특정 사용자의 남용으로 인해 다른 사용자의 서비스 경험이 저하되는 것을 막습니다.
4. **비용 관리:** API 호출 수나 데이터 전송량에 따라 비용이 발생하는 클라우드 환경 또는 외부 유료 API를 사용하는 경우, Rate Limiting은 예측 불가능한 비용 증가를 억제하는 데 도움이 됩니다.
5. **서비스 품질 및 안정성 유지:** API 서버가 처리할 수 있는 부하를 예측 가능한 수준으로 유지함으로써, 응답 시간을 일정하게 유지하고 전반적인 서비스 안정성을 높일 수 있습니다.


### Rate Limiting은 어떻게 동작하나요?

Rate Limiting의 기본 원리는 비교적 간단합니다.

1. **요청 식별:** 각 API 요청을 보낸 주체(클라이언트)를 고유하게 식별합니다. 식별자로는 주로 다음 정보들이 사용됩니다.
    * IP 주소
    * API 키 (인증된 사용자/애플리케이션)
    * 사용자 ID (로그인 기반 서비스)
    * 세션 토큰 또는 JWT(JSON Web Token)
2. **요청 추적:** 식별된 클라이언트별로 특정 시간 간격(예: 1분, 1시간) 동안 들어온 요청 수를 계산하고 추적합니다.
3. **제한 적용:** 추적된 요청 수가 미리 설정된 임계값(Limit)을 초과하면, 해당 클라이언트의 추가적인 API 요청을 일시적으로 **거부(Reject)**합니다. 설정된 시간 간격이 지나면 카운터가 초기화되거나, 시간이 지남에 따라 요청 가능 횟수가 다시 늘어나면 요청을 허용합니다.


### 주요 Rate Limiting 전략 (알고리즘)

Rate Limiting을 구현하는 데는 여러 가지 알고리즘(전략)이 사용되며, 각각 장단점이 있습니다.

* **고정 윈도우 카운터 (Fixed Window Counter):**
  * 가장 간단한 방식입니다. 예를 들어 '매 1분마다 최대 100개 요청 허용'과 같이 고정된 시간 구간(Window) 동안의 요청 수를 셉니다.
  * 구현은 쉽지만, 윈도우의 경계(예: 0초와 59초)에 요청이 몰리면 짧은 순간에 설정된 제한의 최대 2배까지 요청이 처리될 수 있는 단점이 있습니다.
* **슬라이딩 윈도우 로그 (Sliding Window Log):**
  * 각 요청의 정확한 타임스탬프를 로그에 기록합니다. 현재 시간을 기준으로 정확히 과거 특정 시간(예: 최근 60초) 내의 요청 수를 계산하여 제한을 적용합니다.
  * 매우 정확하지만, 모든 요청의 타임스탬프를 저장하고 조회해야 하므로 요청량이 많을 경우 메모리 사용량과 계산 비용이 높을 수 있습니다.
* **슬라이딩 윈도우 카운터 (Sliding Window Counter):**
  * 고정 윈도우의 경계 문제를 완화하고, 슬라이딩 윈도우 로그의 높은 비용 문제를 절충한 방식입니다. 현재 윈도우와 이전 윈도우의 요청 수를 기반으로 현재 요청 허용량을 추정하여 계산 효율성을 높입니다.
* **토큰 버킷 (Token Bucket):**
  * 가상의 '버킷'에 일정한 속도로 '토큰'이 채워집니다. 각 API 요청은 버킷에서 토큰 1개를 소모합니다. 버킷에 토큰이 있어야만 요청이 처리되고, 토큰이 없으면 요청은 거부됩니다.
  * 버킷의 최대 크기만큼의 토큰을 미리 모아둘 수 있어, 짧은 시간 동안 발생하는 요청 폭증(Burst)을 허용하면서도 평균 요청률은 제어할 수 있습니다.
* **누수 버킷 (Leaky Bucket):**
  * 요청이 들어오면 가상의 '버킷'에 쌓이고, 버킷에서는 일정한 속도(누수율)로 요청이 처리됩니다. 버킷이 가득 차면 새로 들어오는 요청은 버려집니다(거부).
  * 요청 처리 속도를 일정하게 유지하여 서버 부하를 안정화시키는 데 효과적이지만, 토큰 버킷과 달리 갑작스러운 요청 폭증을 허용하지 않습니다.


### 구현 시 고려사항

* **구현 위치:** Rate Limiting 로직은 아키텍처의 여러 계층에서 구현될 수 있습니다.
  * **API 게이트웨이:** Nginx, Kong, Apigee, AWS API Gateway, Azure API Management 등 API 요청이 가장 먼저 도달하는 게이트웨이에서 중앙 집중적으로 관리하는 것이 가장 일반적이고 효율적입니다.
  * **로드 밸런서:** 일부 고급 로드 밸런서(ALB, Nginx Plus 등)도 Rate Limiting 기능을 제공합니다.
  * **애플리케이션 미들웨어:** 웹 프레임워크(예: Express.js, Spring Boot, Django)의 미들웨어나 인터셉터/필터 기능을 사용하여 애플리케이션 수준에서 구현할 수 있습니다.
  * **애플리케이션 코드:** 개별 API 엔드포인트 코드 내에서 직접 구현할 수도 있지만, 정책 변경이나 관리가 분산되어 복잡성이 증가할 수 있습니다.
* **데이터 저장소:** 분산 환경(여러 서버 인스턴스)에서는 각 클라이언트의 요청 수를 모든 인스턴스가 공유하고 정확하게 추적해야 합니다. 이를 위해 Redis나 Memcached와 같은 빠르고 원자적 연산(Atomic operation)을 지원하는 인메모리 데이터 저장소를 사용하는 것이 일반적입니다.


### 제한 초과 시 처리 방법

Rate Limit 임계값을 초과한 요청에 대해서는 클라이언트가 상황을 인지하고 적절히 대응할 수 있도록 명확하게 알려주는 것이 중요합니다.

* **HTTP 상태 코드:** 표준적으로 **`429 Too Many Requests`** 상태 코드를 반환해야 합니다.
* **`Retry-After` 헤더:** 클라이언트가 얼마 후에 다시 요청을 시도할 수 있는지 알려주는 `Retry-After` HTTP 응답 헤더를 포함하는 것이 매우 권장됩니다. 값은 재시도까지 기다려야 하는 **초 단위** (예: `Retry-After: 60`) 또는 요청이 다시 허용될 **정확한 날짜/시간** (HTTP-date 형식)이 될 수 있습니다.
* **오류 메시지:** 응답 본문(Body)에 속도 제한에 도달했음을 설명하는 명확하고 이해하기 쉬운 메시지를 포함하는 것이 사용자 경험에 도움이 됩니다.


### Rate Limiting vs. Throttling

Rate Limiting과 유사하지만 약간 다른 개념으로 **Throttling(스로틀링)**이 있습니다.

* **Rate Limiting:** 주로 설정된 요청 수 **임계값을 넘는 요청을 거부(Reject)**하는 데 초점을 맞춥니다. 시스템 보호와 공정성 유지가 주 목적입니다.
* **Throttling:** 요청 처리 **속도 자체를 제어**하는 데 중점을 둡니다. 요청이 너무 많을 경우 즉시 거부하기보다는, 처리를 잠시 **지연(Delay)**시키거나 요청을 큐에 넣어 정해진 속도로 순차 처리함으로써 트래픽 흐름을 부드럽게 조절합니다.
* 실제 시스템에서는 두 가지 방식의 목표와 효과가 겹치는 부분이 있어 혼용되어 사용되거나, 같은 의미로 사용되기도 합니다. 중요한 것은 시스템의 요구사항에 맞게 요청 빈도를 제어하는 메커니즘을 구현하는 것입니다.


### Rate Limiting 모범 사례

* **명확한 정책 문서화:** API 사용자(개발자)에게 적용되는 Rate Limit 정책(제한 기준, 시간 단위, 임계값, 초과 시 동작 등)을 API 문서 등을 통해 명확하게 안내해야 합니다.
* **적절한 임계값 설정:** 서비스의 특성, 예상되는 정상 트래픽 패턴, 서버의 실제 처리 용량 등을 종합적으로 고려하여 현실적인 임계값을 설정해야 합니다. API 엔드포인트의 중요도나 자원 소모량, 사용자 등급(예: 무료/유료 플랜, 내부/외부 사용자)에 따라 차등적인 제한을 두는 것이 효과적입니다.
* **모니터링 및 알림:** 어떤 클라이언트가 얼마나 자주 Rate Limit에 도달하는지 모니터링하고 로그를 남기는 것이 중요합니다. 이를 통해 서비스 사용 패턴을 이해하고, 임계값 설정이 적절한지 검토하며, 비정상적인 트래픽이나 잠재적인 남용/공격 시도를 조기에 감지하고 알림을 받도록 설정해야 합니다.
* **유용한 오류 응답 제공:** `429` 상태 코드와 함께 `Retry-After` 헤더를 정확히 제공하여 클라이언트 측에서 재시도 로직을 구현하기 용이하게 만들어야 합니다.
* **유연성 고려:** 모든 상황에 엄격하게 동일한 제한을 적용하기보다, 특정 파트너나 중요한 사용 사례에 대해서는 예외적으로 제한을 완화하거나 일시적으로 조정할 수 있는 메커니즘을 고려할 수 있습니다. (단, 남용되지 않도록 주의해야 합니다.)
* **점진적 적용 및 테스트:** 새로운 Rate Limit 정책을 도입할 때는 실제 사용자 트래픽에 미치는 영향을 최소화하기 위해 낮은 비율의 트래픽에 먼저 적용하거나, 경고(Logging)만 하고 실제 차단은 하지 않는 모드(Dry run)로 시작하여 충분히 테스트하고 점진적으로 확대하는 것이 안전합니다.


## Express.js에서 Rate Limiting 구현

`express-rate-limit` 패키지를 사용하여 Express.js 애플리케이션에 Rate Limiting을 쉽게 구현할 수 있습니다. 이 패키지는 고정 윈도우 카운터 방식으로 Rate Limiting을 지원합니다.


```typescript
import express from 'express';
import rateLimit from 'express-rate-limit';

const app = express();

const limiter = rateLimit({
  windowMs: 1 * 60 * 1000, // 1분
  max: 100, // 각 IP당 최대 요청 수
  message: '요청이 너무 많습니다. 잠시 후 다시 시도해주세요.',
  headers: true, // Rate Limit 관련 헤더 포함 여부
});

app.use(limiter); // 모든 요청에 Rate Limiting 적용

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`서버가 ${PORT} 포트에서 실행 중입니다.`);
});
```
